# Optimization-Algorithms

In this repository, I've tried to implement popular optimization algorithms used in AI: _Gradient Descent_, _Gradient Descent with Momentum_, _AdaGrad_, _RMProps_, _Adam_. I also have tried to visualize to see how the values of function are optimized.

Reference:
- [Gradient Descent](https://machinelearningmastery.com/gradient-descent-optimization-from-scratch/).
- [Gradient Descent with Momentum](https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/).
- [AdaGrad optimizer](https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/).
- [RMProps optimizer](https://machinelearningmastery.com/gradient-descent-with-rmsprop-from-scratch/).
- [Adam optimizer](https://machinelearningmastery.com/adam-optimization-from-scratch/).
